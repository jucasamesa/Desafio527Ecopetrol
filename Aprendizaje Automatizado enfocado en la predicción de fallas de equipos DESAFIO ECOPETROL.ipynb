{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Automatizado enfocado en la predicción de fallas de equipos DESAFIO ECOPETROL\n",
    "\n",
    "### Desafío 000527: ¿Cómo aportar a la transformación energética, dinámicas sociales, culturales o tecnológicas?\n",
    "\n",
    "## Machine Learning for Equipment Failure Prediction and Equipment Maintenance (PM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al trabajar con máquinas o instrumentos que requieren mantenimiento periódico, hay generalmente 3 posibles sucesos:\n",
    "\n",
    "1. Puedes realizar realizar un mantenimiento demasiado frecuente. Es decir, se realiza mantenimiento cuando no es requerido. En este escenario estas perdiendo dinero. (Mantenimiento Preventivo No Eficiente)\n",
    "\n",
    "1. No realizas mantenimiento con suficiente frecuencia. La máquina o instrumento terminara dañandose causando afectaciones a personas, ambiente y generando costos adicionales en reparación, producción, etc. (Mantenimiento Correctivo)\n",
    "\n",
    "1. Se realiza el mantenimiento cuando se requiere. Mejor opción pero díficil de predecir el momento justo para realizar el mantenimiento. (Mantenimiento Preventivo Eficiente)\n",
    "\n",
    "Los datos utilizados son artificiales pero basados en la experiencia de 4 años de estudio de fallas de equipos realizado por [Shad Griffin](https://medium.com/swlh/machine-learning-for-equipment-failure-prediction-and-predictive-maintenance-pm-e72b1ce42da1#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La compañía problema o del caso de estudio posee datos que incluyen 419 máquinas que fallaron en el transcurso de 2 años. Generando 11.7 millones de dolares en gasto de mantenimiento. La mayoría de estos costos fueron de mantenimiento tipo correctivo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actualmente le cuesta el mantenimiento a la compañía $28,000 USD por falla o por mantenimiento por máquina. \n",
    "\n",
    "**Meta: Bajar costo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='0'>Tabla de Contenidos</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Carga de Datos](#setup1)<br>\n",
    " \n",
    "2. [Exploración de Datos](#explore)<br>\n",
    "\n",
    "3. [Transformación de Datos e Ingeniería de Características](#trans)<br>\n",
    " \n",
    "4. [Manejando el pequeño número de fallas](#small)<br>\n",
    "    4.1 [Expandiendo la ventana de fallas](#window)<br>\n",
    "    4.2 [Creación de pruebas, entrenamiento y grupos de validación](#groups)<br>\n",
    "    4.3 [SMOTE the Training Data](#smote)<br>\n",
    "5. [More Data Transformations and Feature Engineering](#more)<br>\n",
    "6. [Build the Model on the Balanced Data Set](#build)<br>\n",
    "7. [Score the Unbalanced Training Data Set](#score)<br>\n",
    "8. [Business Rules and Heuristics](#bus)<br>\n",
    "9. [Define a True Positive, True Negative, False Positive and False Negative](#tp)<br>\n",
    "10. [Apply Model and Heuristics to the Testing and Validation Data Sets](#apply)<br>\n",
    "11. [Conclusions](#conc)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Carga de Datos <a id=\"setup1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Se instalan e importan las librerias necesarias para el proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.23.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.19.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.17.0)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn>=0.23->imbalanced-learn->imblearn) (2.1.0)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.7.0 imblearn-0.0\n",
      "Requirement already up-to-date: plotly in c:\\users\\user\\anaconda3\\lib\\site-packages (4.14.3)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from plotly) (1.3.3)\n",
      "Collecting chart-studio\n",
      "  Downloading chart_studio-1.1.0-py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied, skipping upgrade: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from chart-studio) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from chart-studio) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: retrying>=1.3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from chart-studio) (1.3.3)\n",
      "Requirement already satisfied, skipping upgrade: plotly in c:\\users\\user\\anaconda3\\lib\\site-packages (from chart-studio) (4.14.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->chart-studio) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->chart-studio) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->chart-studio) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->chart-studio) (1.25.11)\n",
      "Installing collected packages: chart-studio\n",
      "Successfully installed chart-studio-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn --upgrade\n",
    "!pip install plotly --upgrade\n",
    "!pip install chart-studio --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly as plotly\n",
    "import pandas as pd\n",
    "#from botocore.client import Config\n",
    "#import ibm_boto3\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import types\n",
    "import pandas as pd\n",
    "#from botocore.client import Config\n",
    "#import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importar la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir csv a un  df de pandas\n",
    "df1 = pd.read_csv(r\"C:\\Users\\user\\Desktop\\Profundización DS\\Desafio Ecopetrol\\equipment_failure_data_1.csv\", sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir csv a un  df de pandas\n",
    "df2 = pd.read_csv(r\"C:\\Users\\user\\Desktop\\Profundización DS\\Desafio Ecopetrol\\equipment_failure_data_2.csv\", sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenación de los dos df en uno\n",
    "df=pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#0\"><font size=\"1\">Volver al inicio</font></a>\n",
    "## 2.0 Exploración de Datos <a id=\"explore\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>REGION_CLUSTER</th>\n",
       "      <th>MAINTENANCE_VENDOR</th>\n",
       "      <th>MANUFACTURER</th>\n",
       "      <th>WELL_GROUP</th>\n",
       "      <th>S15</th>\n",
       "      <th>S17</th>\n",
       "      <th>S13</th>\n",
       "      <th>S5</th>\n",
       "      <th>S16</th>\n",
       "      <th>S19</th>\n",
       "      <th>S18</th>\n",
       "      <th>EQUIPMENT_FAILURE</th>\n",
       "      <th>S8</th>\n",
       "      <th>AGE_OF_EQUIPMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>12/2/14</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>11.088000</td>\n",
       "      <td>145.223448</td>\n",
       "      <td>39.34</td>\n",
       "      <td>3501.0</td>\n",
       "      <td>8.426869</td>\n",
       "      <td>1.9</td>\n",
       "      <td>24.610345</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>12/3/14</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>8.877943</td>\n",
       "      <td>187.573214</td>\n",
       "      <td>39.20</td>\n",
       "      <td>3489.0</td>\n",
       "      <td>6.483714</td>\n",
       "      <td>1.9</td>\n",
       "      <td>24.671429</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100001</td>\n",
       "      <td>12/4/14</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>8.676444</td>\n",
       "      <td>148.363704</td>\n",
       "      <td>38.87</td>\n",
       "      <td>3459.0</td>\n",
       "      <td>6.159659</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.733333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100001</td>\n",
       "      <td>12/5/14</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>9.988338</td>\n",
       "      <td>133.660000</td>\n",
       "      <td>39.47</td>\n",
       "      <td>3513.0</td>\n",
       "      <td>9.320308</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.773077</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100001</td>\n",
       "      <td>12/6/14</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>Y</td>\n",
       "      <td>1</td>\n",
       "      <td>8.475264</td>\n",
       "      <td>197.181600</td>\n",
       "      <td>40.33</td>\n",
       "      <td>3589.0</td>\n",
       "      <td>8.022960</td>\n",
       "      <td>1.5</td>\n",
       "      <td>24.808000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID     DATE REGION_CLUSTER MAINTENANCE_VENDOR MANUFACTURER  WELL_GROUP  \\\n",
       "0  100001  12/2/14              G                  O            Y           1   \n",
       "1  100001  12/3/14              G                  O            Y           1   \n",
       "2  100001  12/4/14              G                  O            Y           1   \n",
       "3  100001  12/5/14              G                  O            Y           1   \n",
       "4  100001  12/6/14              G                  O            Y           1   \n",
       "\n",
       "         S15         S17    S13      S5       S16  S19        S18  \\\n",
       "0  11.088000  145.223448  39.34  3501.0  8.426869  1.9  24.610345   \n",
       "1   8.877943  187.573214  39.20  3489.0  6.483714  1.9  24.671429   \n",
       "2   8.676444  148.363704  38.87  3459.0  6.159659  2.0  24.733333   \n",
       "3   9.988338  133.660000  39.47  3513.0  9.320308  2.0  24.773077   \n",
       "4   8.475264  197.181600  40.33  3589.0  8.022960  1.5  24.808000   \n",
       "\n",
       "   EQUIPMENT_FAILURE   S8  AGE_OF_EQUIPMENT  \n",
       "0                  0  0.0               880  \n",
       "1                  0  0.0               881  \n",
       "2                  0  0.0               882  \n",
       "3                  0  0.0               883  \n",
       "4                  0  0.0               884  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se explica la metadata de las columnas:\n",
    "\n",
    "ID — Este campo representa una máquina en especifico.\n",
    "\n",
    "DATE — Fecha de la observación.\n",
    "\n",
    "REGION_CLUSTER — Campo que representa la región en la cuál la máquina se encuentra.\n",
    "\n",
    "MAINTENANCE_VENDOR — Representa la compañía que provee el mantenimiento y servicio de la máquina.\n",
    "\n",
    "MANUFACTURER — Compañía que manufacturó el equipo.\n",
    "\n",
    "WELL_GROUP — Representa el tipo de máquina.\n",
    "\n",
    "EQUIPMENT_AGE — Edad de la máquina en días.\n",
    "\n",
    "S15 — Valor del sensor.\n",
    "\n",
    "S17 — Valor del sensor.\n",
    "\n",
    "S13 — Valor del sensor.\n",
    "\n",
    "S16 — Valor del sensor.\n",
    "\n",
    "S19 — Valor del sensor.\n",
    "\n",
    "S18 — Valor del sensor.\n",
    "\n",
    "S8 — Valor del sensor.\n",
    "\n",
    "EQUIPMENT_FAILURE — Un ‘1’ significa que el equipo ha fallado y un ‘0’ que no lo ha hecho.\n",
    "\n",
    "Nuestra meta es construir un modelo que prediga la falla del equipo, es decir, se utilizaran las otras variables para predecir EQUIPMENT_FAILURE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinamos la cantidad de datos, filas y columnas. Son 307.751 filas y 16 columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307751, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 421 máquinas en la base de datos (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ID.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 731 datos unicos en la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>REGION_CLUSTER</th>\n",
       "      <th>MAINTENANCE_VENDOR</th>\n",
       "      <th>MANUFACTURER</th>\n",
       "      <th>WELL_GROUP</th>\n",
       "      <th>S15</th>\n",
       "      <th>S17</th>\n",
       "      <th>S13</th>\n",
       "      <th>S5</th>\n",
       "      <th>S16</th>\n",
       "      <th>S19</th>\n",
       "      <th>S18</th>\n",
       "      <th>EQUIPMENT_FAILURE</th>\n",
       "      <th>S8</th>\n",
       "      <th>AGE_OF_EQUIPMENT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100066</th>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100310</th>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100503</th>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100530</th>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100519</th>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "      <td>731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE REGION_CLUSTER MAINTENANCE_VENDOR MANUFACTURER WELL_GROUP   S15  \\\n",
       "       count          count              count        count      count count   \n",
       "ID                                                                             \n",
       "100066   731            731                731          731        731   731   \n",
       "100310   731            731                731          731        731   731   \n",
       "100503   731            731                731          731        731   731   \n",
       "100530   731            731                731          731        731   731   \n",
       "100519   731            731                731          731        731   731   \n",
       "\n",
       "         S17   S13    S5   S16   S19   S18 EQUIPMENT_FAILURE    S8  \\\n",
       "       count count count count count count             count count   \n",
       "ID                                                                   \n",
       "100066   731   731   731   731   731   731               731   731   \n",
       "100310   731   731   731   731   731   731               731   731   \n",
       "100503   731   731   731   731   731   731               731   731   \n",
       "100530   731   731   731   731   731   731               731   731   \n",
       "100519   731   731   731   731   731   731               731   731   \n",
       "\n",
       "       AGE_OF_EQUIPMENT  \n",
       "                  count  \n",
       "ID                       \n",
       "100066              731  \n",
       "100310              731  \n",
       "100503              731  \n",
       "100530              731  \n",
       "100519              731  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idC = pd.DataFrame(df.groupby(['ID']).agg(['count']))\n",
    "idC.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>REGION_CLUSTER</th>\n",
       "      <th>MAINTENANCE_VENDOR</th>\n",
       "      <th>MANUFACTURER</th>\n",
       "      <th>WELL_GROUP</th>\n",
       "      <th>S15</th>\n",
       "      <th>S17</th>\n",
       "      <th>S13</th>\n",
       "      <th>S5</th>\n",
       "      <th>S16</th>\n",
       "      <th>S19</th>\n",
       "      <th>S18</th>\n",
       "      <th>EQUIPMENT_FAILURE</th>\n",
       "      <th>S8</th>\n",
       "      <th>AGE_OF_EQUIPMENT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8/5/16</th>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4/13/16</th>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1/19/16</th>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2/11/15</th>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10/12/15</th>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "      <td>421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID REGION_CLUSTER MAINTENANCE_VENDOR MANUFACTURER WELL_GROUP  \\\n",
       "         count          count              count        count      count   \n",
       "DATE                                                                       \n",
       "8/5/16     421            421                421          421        421   \n",
       "4/13/16    421            421                421          421        421   \n",
       "1/19/16    421            421                421          421        421   \n",
       "2/11/15    421            421                421          421        421   \n",
       "10/12/15   421            421                421          421        421   \n",
       "\n",
       "           S15   S17   S13    S5   S16   S19   S18 EQUIPMENT_FAILURE    S8  \\\n",
       "         count count count count count count count             count count   \n",
       "DATE                                                                         \n",
       "8/5/16     421   421   421   421   421   421   421               421   421   \n",
       "4/13/16    421   421   421   421   421   421   421               421   421   \n",
       "1/19/16    421   421   421   421   421   421   421               421   421   \n",
       "2/11/15    421   421   421   421   421   421   421               421   421   \n",
       "10/12/15   421   421   421   421   421   421   421               421   421   \n",
       "\n",
       "         AGE_OF_EQUIPMENT  \n",
       "                    count  \n",
       "DATE                       \n",
       "8/5/16                421  \n",
       "4/13/16               421  \n",
       "1/19/16               421  \n",
       "2/11/15               421  \n",
       "10/12/15              421  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dateC = pd.DataFrame(df.groupby(['DATE']).agg(['count']))\n",
    "dateC.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos 731 datos únicos.  Entonces si tenemos 421 máquinas y 731 datos únicos, deberíamos tener 307.751 registros. Basados en el comando **.shape**, tenemos 1 registro por máquina por valor de registro. No hay duplicados en el **df**. De todas formas se utilizaran las funciones para comprobar esto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    307751\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicates = df.duplicated()\n",
    "duplicates.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora buscaremos valores no registrados en los campos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                    0\n",
       "DATE                  0\n",
       "REGION_CLUSTER        0\n",
       "MAINTENANCE_VENDOR    0\n",
       "MANUFACTURER          0\n",
       "WELL_GROUP            0\n",
       "S15                   0\n",
       "S17                   0\n",
       "S13                   0\n",
       "S5                    0\n",
       "S16                   0\n",
       "S19                   0\n",
       "S18                   0\n",
       "EQUIPMENT_FAILURE     0\n",
       "S8                    0\n",
       "AGE_OF_EQUIPMENT      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora examinamos la variable dependiente en más detalle. De los 307.751 registros, solo ocurrieron 421 fallas. Es decir, hay una tasa de falla del .14%. En otras palabras, por cada falla hay un poco más de 700 no-fallas. \n",
    "**Esto quiere decir que la base de datos está muy desbalanceada.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.136986301369863"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PFalla = df.EQUIPMENT_FAILURE.value_counts()[1]*100/df.EQUIPMENT_FAILURE.value_counts()[0]\n",
    "PFalla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pueden explorar las variables numéricas descriptivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>WELL_GROUP</th>\n",
       "      <th>S15</th>\n",
       "      <th>S17</th>\n",
       "      <th>S13</th>\n",
       "      <th>S5</th>\n",
       "      <th>S16</th>\n",
       "      <th>S19</th>\n",
       "      <th>S18</th>\n",
       "      <th>EQUIPMENT_FAILURE</th>\n",
       "      <th>S8</th>\n",
       "      <th>AGE_OF_EQUIPMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "      <td>307751.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>100310.826603</td>\n",
       "      <td>4.543943</td>\n",
       "      <td>14.585192</td>\n",
       "      <td>80.265541</td>\n",
       "      <td>35.018249</td>\n",
       "      <td>4675.848252</td>\n",
       "      <td>7.972097</td>\n",
       "      <td>9.069123</td>\n",
       "      <td>137.963064</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>144.665715</td>\n",
       "      <td>2524.192399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>177.574390</td>\n",
       "      <td>2.284121</td>\n",
       "      <td>8.817056</td>\n",
       "      <td>85.804273</td>\n",
       "      <td>14.446585</td>\n",
       "      <td>2521.074632</td>\n",
       "      <td>2.321949</td>\n",
       "      <td>16.898887</td>\n",
       "      <td>238.890128</td>\n",
       "      <td>0.036961</td>\n",
       "      <td>240.773926</td>\n",
       "      <td>3158.930976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>100001.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-16.490000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100161.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.694100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>3209.000000</td>\n",
       "      <td>6.621500</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>11.798276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.250000</td>\n",
       "      <td>721.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>100311.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.661600</td>\n",
       "      <td>31.680000</td>\n",
       "      <td>34.940000</td>\n",
       "      <td>4237.047619</td>\n",
       "      <td>8.004000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>38.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.080000</td>\n",
       "      <td>1113.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>100467.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>22.560000</td>\n",
       "      <td>160.080000</td>\n",
       "      <td>41.610000</td>\n",
       "      <td>5743.000000</td>\n",
       "      <td>9.460000</td>\n",
       "      <td>10.600000</td>\n",
       "      <td>150.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>165.092608</td>\n",
       "      <td>2784.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100617.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>59.040000</td>\n",
       "      <td>2555.520000</td>\n",
       "      <td>592.890000</td>\n",
       "      <td>52767.000000</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>4151.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2068.110000</td>\n",
       "      <td>15170.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ID     WELL_GROUP            S15            S17  \\\n",
       "count  307751.000000  307751.000000  307751.000000  307751.000000   \n",
       "mean   100310.826603       4.543943      14.585192      80.265541   \n",
       "std       177.574390       2.284121       8.817056      85.804273   \n",
       "min    100001.000000       1.000000       0.000000       0.000000   \n",
       "25%    100161.000000       3.000000       7.694100       0.000000   \n",
       "50%    100311.000000       5.000000      11.661600      31.680000   \n",
       "75%    100467.000000       6.000000      22.560000     160.080000   \n",
       "max    100617.000000       8.000000      59.040000    2555.520000   \n",
       "\n",
       "                 S13             S5            S16            S19  \\\n",
       "count  307751.000000  307751.000000  307751.000000  307751.000000   \n",
       "mean       35.018249    4675.848252       7.972097       9.069123   \n",
       "std        14.446585    2521.074632       2.321949      16.898887   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%        28.200000    3209.000000       6.621500       0.900000   \n",
       "50%        34.940000    4237.047619       8.004000       4.200000   \n",
       "75%        41.610000    5743.000000       9.460000      10.600000   \n",
       "max       592.890000   52767.000000      24.600000     511.000000   \n",
       "\n",
       "                 S18  EQUIPMENT_FAILURE             S8  AGE_OF_EQUIPMENT  \n",
       "count  307751.000000      307751.000000  307751.000000     307751.000000  \n",
       "mean      137.963064           0.001368     144.665715       2524.192399  \n",
       "std       238.890128           0.036961     240.773926       3158.930976  \n",
       "min         0.000000           0.000000     -16.490000          0.000000  \n",
       "25%        11.798276           0.000000       9.250000        721.000000  \n",
       "50%        38.200000           0.000000      53.080000       1113.000000  \n",
       "75%       150.900000           0.000000     165.092608       2784.000000  \n",
       "max      4151.700000           1.000000    2068.110000      15170.000000  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se examina una correlación simple de la variable independiente con la dependiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EQUIPMENT_FAILURE</th>\n",
       "      <th>ABS_EQUIPMENT_FAILURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EQUIPMENT_FAILURE</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S15</th>\n",
       "      <td>-6.036352e-02</td>\n",
       "      <td>6.036352e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S17</th>\n",
       "      <td>-3.429070e-02</td>\n",
       "      <td>3.429070e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S18</th>\n",
       "      <td>9.765002e-03</td>\n",
       "      <td>9.765002e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S13</th>\n",
       "      <td>-8.617761e-03</td>\n",
       "      <td>8.617761e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S5</th>\n",
       "      <td>-7.189979e-03</td>\n",
       "      <td>7.189979e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S8</th>\n",
       "      <td>6.517148e-03</td>\n",
       "      <td>6.517148e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S16</th>\n",
       "      <td>-6.138895e-03</td>\n",
       "      <td>6.138895e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S19</th>\n",
       "      <td>-6.087474e-03</td>\n",
       "      <td>6.087474e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE_OF_EQUIPMENT</th>\n",
       "      <td>4.733368e-04</td>\n",
       "      <td>4.733368e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WELL_GROUP</th>\n",
       "      <td>7.048348e-17</td>\n",
       "      <td>7.048348e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>2.959871e-18</td>\n",
       "      <td>2.959871e-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   EQUIPMENT_FAILURE  ABS_EQUIPMENT_FAILURE\n",
       "EQUIPMENT_FAILURE       1.000000e+00           1.000000e+00\n",
       "S15                    -6.036352e-02           6.036352e-02\n",
       "S17                    -3.429070e-02           3.429070e-02\n",
       "S18                     9.765002e-03           9.765002e-03\n",
       "S13                    -8.617761e-03           8.617761e-03\n",
       "S5                     -7.189979e-03           7.189979e-03\n",
       "S8                      6.517148e-03           6.517148e-03\n",
       "S16                    -6.138895e-03           6.138895e-03\n",
       "S19                    -6.087474e-03           6.087474e-03\n",
       "AGE_OF_EQUIPMENT        4.733368e-04           4.733368e-04\n",
       "WELL_GROUP              7.048348e-17           7.048348e-17\n",
       "ID                      2.959871e-18           2.959871e-18"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr=df.corr(method='pearson')\n",
    "\n",
    "corr=corr[['EQUIPMENT_FAILURE']]\n",
    "corr['ABS_EQUIPMENT_FAILURE']=abs(corr['EQUIPMENT_FAILURE'])\n",
    "corr=corr.sort_values(by=['ABS_EQUIPMENT_FAILURE'], ascending=[False])\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#0\"><font size=\"1\">Volver al inicio</font></a>\n",
    "## 3.0 Transformación de Datos e Ingeniería de Características <a id=\"trans\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can transform our data for a machine learning model. Specifically, we will create running summaries of the sensor values. Running summaries of sensor values are often useful in predicting equipment failure. For example, if a temperature gauge indicates a machine is warmer than average for the last five days, it may mean something is wrong.\n",
    "\n",
    "Remember that we are working with a panel data set. That is, we have multiple machines measured over two years. As we create our running summaries, we have to make sure that our summaries do not include more than one machine. For example, if we create a ten-day moving average, we do not want the first nine days of a machine to have values from the previous machine.\n",
    "\n",
    "Note that I create twenty-one-day summaries in this example. This works for this use case, but it may be advantageous to use more or different time intervals for other situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dates from character to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd_data['DATE'] = pd.to_datetime(pd_data['DATE'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new field called “flipper” that indicates when the id changes as the data are sorted by ID and DATE in ascending order. We will use this in a few other transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data=pd_data.sort_values(by=['ID','DATE'], ascending=[True, True])\n",
    "\n",
    "pd_data['flipper'] = np.where((pd_data.ID != pd_data.ID.shift(1)), 1, 0)\n",
    "pd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running summaries are often useful transformations for these types of a problems.  For example, a running mean would be the average value over the last x days.  X in this case is the feature window.  The feature window is a parameter that depends on the context of the business problem.  I am setting the value to 21 days, but this may or may not work for your business problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define your feature window. This is the window by which we will aggregate our sensor values.\n",
    "feature_window=21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of days from the first day a machine appears to the current day. This field will be called “TIME_SINCE_START” Also, create a variable called “too_soon.” When “too_soon” is equal to 1, we have less than 21 days (feature_window) of history for the machine.\n",
    "\n",
    "We will use these new variables to create a running mean, median, max, and min. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx=pd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the first record of each machine\n",
    "\n",
    "starter=dfx[dfx['flipper'] == 1]\n",
    "\n",
    "starter=starter[['DATE','ID']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename data to start_date\n",
    "starter=starter.rename(index=str, columns={\"DATE\": \"START_DATE\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert START_DATE to date\n",
    "starter['START_DATE'] = pd.to_datetime(starter['START_DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge START_DATE to the original data set\n",
    "\n",
    "dfx=dfx.sort_values(by=['ID', 'DATE'], ascending=[True, True])\n",
    "starter=starter.sort_values(by=['ID'], ascending=[True])\n",
    "dfx =dfx.merge(starter, on=['ID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of days since the beginning of each well. \n",
    "dfx['C'] = dfx['DATE'] - dfx['START_DATE']\n",
    "dfx['TIME_SINCE_START'] = dfx['C'] / np.timedelta64(1, 'D')\n",
    "dfx=dfx.drop(columns=['C'])\n",
    "dfx['too_soon'] = np.where((dfx.TIME_SINCE_START < feature_window) , 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a running mean, max, min and median for the sensor variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfx['S5_mean'] = np.where((dfx.too_soon == 0),(dfx['S5'].rolling(min_periods=1, window=feature_window).mean()) , dfx.S5)\n",
    "dfx['S5_median'] = np.where((dfx.too_soon == 0),(dfx['S5'].rolling(min_periods=1, window=feature_window).median()) , dfx.S5)\n",
    "dfx['S5_max'] = np.where((dfx.too_soon == 0),(dfx['S5'].rolling(min_periods=1, window=feature_window).max()) , dfx.S5)\n",
    "dfx['S5_min'] = np.where((dfx.too_soon == 0),(dfx['S5'].rolling(min_periods=1, window=feature_window).min()) , dfx.S5)\n",
    "\n",
    "\n",
    "dfx['S13_mean'] = np.where((dfx.too_soon == 0),(dfx['S13'].rolling(min_periods=1, window=feature_window).mean()) , dfx.S13)\n",
    "dfx['S13_median'] = np.where((dfx.too_soon == 0),(dfx['S13'].rolling(min_periods=1, window=feature_window).median()) , dfx.S13)\n",
    "dfx['S13_max'] = np.where((dfx.too_soon == 0),(dfx['S13'].rolling(min_periods=1, window=feature_window).max()) , dfx.S13)\n",
    "dfx['S13_min'] = np.where((dfx.too_soon == 0),(dfx['S13'].rolling(min_periods=1, window=feature_window).min()) , dfx.S13)\n",
    "\n",
    "\n",
    "dfx['S15_mean'] = np.where((dfx.too_soon == 0),(dfx['S15'].rolling(min_periods=1, window=feature_window).mean()) , dfx.S15)\n",
    "dfx['S15_median'] = np.where((dfx.too_soon == 0),(dfx['S15'].rolling(min_periods=1, window=feature_window).median()) , dfx.S15)\n",
    "dfx['S15_max'] = np.where((dfx.too_soon == 0),(dfx['S15'].rolling(min_periods=1, window=feature_window).max()) , dfx.S15)\n",
    "dfx['S15_min'] = np.where((dfx.too_soon == 0),(dfx['S15'].rolling(min_periods=1, window=feature_window).min()) , dfx.S15)\n",
    "\n",
    "dfx['S16_mean'] = np.where((dfx.too_soon == 0),(dfx['S16'].rolling(min_periods=1, window=feature_window).mean()) , dfx.S16)\n",
    "dfx['S16_median'] = np.where((dfx.too_soon == 0),(dfx['S16'].rolling(min_periods=1, window=feature_window).median()) , dfx.S16)\n",
    "dfx['S16_max'] = np.where((dfx.too_soon == 0),(dfx['S16'].rolling(min_periods=1, window=feature_window).max()) , dfx.S16)\n",
    "dfx['S16_min'] = np.where((dfx.too_soon == 0),(dfx['S16'].rolling(min_periods=1, window=feature_window).min()) , dfx.S16)\n",
    "\n",
    "\n",
    "dfx['S17_mean'] = np.where((dfx.too_soon == 0),(dfx['S17'].rolling(min_periods=1, window=feature_window).mean()) , dfx.S17)\n",
    "dfx['S17_median'] = np.where((dfx.too_soon == 0),(dfx['S17'].rolling(min_periods=1, window=feature_window).median()) , dfx.S17)\n",
    "dfx['S17_max'] = np.where((dfx.too_soon == 0),(dfx['S17'].rolling(min_periods=1, window=feature_window).max()) , dfx.S17)\n",
    "dfx['S17_min'] = np.where((dfx.too_soon == 0),(dfx['S17'].rolling(min_periods=1, window=feature_window).min()) , dfx.S17)\n",
    "\n",
    "dfx['S18_mean'] = np.where((dfx.too_soon == 0),(dfx['S18'].rolling(min_periods=1, window=feature_window).mean()) , dfx.S18)\n",
    "dfx['S18_median'] = np.where((dfx.too_soon == 0),(dfx['S18'].rolling(min_periods=1, window=feature_window).median()) , dfx.S18)\n",
    "dfx['S18_max'] = np.where((dfx.too_soon == 0),(dfx['S18'].rolling(min_periods=1, window=feature_window).max()) , dfx.S18)\n",
    "dfx['S18_min'] = np.where((dfx.too_soon == 0),(dfx['S18'].rolling(min_periods=1, window=feature_window).min()) , dfx.S18)\n",
    "\n",
    "\n",
    "\n",
    "dfx['S19_mean'] = np.where((dfx.too_soon == 0),(dfx['S19'].rolling(min_periods=1, window=feature_window).mean()) , dfx.S19)\n",
    "dfx['S19_median'] = np.where((dfx.too_soon == 0),(dfx['S19'].rolling(min_periods=1, window=feature_window).median()) , dfx.S19)\n",
    "dfx['S19_max'] = np.where((dfx.too_soon == 0),(dfx['S19'].rolling(min_periods=1, window=feature_window).max()) , dfx.S19)\n",
    "dfx['S19_min'] = np.where((dfx.too_soon == 0),(dfx['S19'].rolling(min_periods=1, window=feature_window).min()) , dfx.S19)\n",
    "\n",
    "\n",
    "dfx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful transformation is to look for sudden spikes in sensor values. This code creates a value indicating how far the current value is from the immediate norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfx['S5_chg'] = np.where((dfx.S5_mean == 0),0 , dfx.S5/dfx.S5_mean)\n",
    "\n",
    "\n",
    "dfx['S13_chg'] = np.where((dfx.S13_mean == 0),0 , dfx.S13/dfx.S13_mean)\n",
    "\n",
    "dfx['S15_chg'] = np.where((dfx.S15_mean==0),0 , dfx.S15/dfx.S15_mean)\n",
    "dfx['S16_chg'] = np.where((dfx.S16_mean == 0),0 , dfx.S16/dfx.S16_mean)\n",
    "dfx['S17_chg'] = np.where((dfx.S17_mean == 0),0 , dfx.S17/dfx.S17_mean)\n",
    "dfx['S18_chg'] = np.where((dfx.S18_mean == 0),0 , dfx.S18/dfx.S18_mean)\n",
    "dfx['S19_chg'] = np.where((dfx.S19_mean == 0),0 , dfx.S19/dfx.S19_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the data set to the original name\n",
    "pd_data=dfx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Dealing with the small number of failures. <a id=\"small\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Expand the Failure (Target) Window <a id=\"window\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machines are engineered to last. If something breaks all the time, you won’t buy it, would you?\n",
    "\n",
    "Because machines generally last a long time, we typically do not have many examples of failure. This means the data sets we use in PM are almost always unbalanced. \n",
    "\n",
    "One way to increase the number of failures is to expand the failure or target window. That is, make the dependent variable, not just the day the equipment failed but the 30 days (or another appropriate interval) leading up to the failure.\n",
    "\n",
    "In this example, I use a 28-day target window. We will use the 28 days leading up to a failure as the dependent variable in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_window=28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the data by ID and DATE.  Make sure the index reflects this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_data=pd_data.sort_values(by=['ID', 'DATE'], ascending=[True, True])\n",
    "pd_data.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new data frame that contains the failure records.  Rename DATE to FAILURE_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_failure_thingy=pd_data[pd_data['EQUIPMENT_FAILURE'] == 1]\n",
    "\n",
    "df_failure_thingy=df_failure_thingy[['DATE','ID']]\n",
    "\n",
    "df_failure_thingy=df_failure_thingy.rename(index=str, columns={\"DATE\": \"FAILURE_DATE\"})\n",
    "\n",
    "pd_data=pd_data.sort_values(by=['ID'], ascending=[True])\n",
    "df_failure_thingy=df_failure_thingy.sort_values(by=['ID'], ascending=[True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the FAILURE_DATE to each ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd_data =pd_data.merge(df_failure_thingy, on=['ID'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each record calculate the number of days until failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd_data=pd_data.sort_values(by=['ID','DATE'], ascending=[True, True])\n",
    "\n",
    "pd_data['FAILURE_DATE'] = pd.to_datetime(pd_data['FAILURE_DATE'])\n",
    "pd_data['DATE'] = pd.to_datetime(pd_data['DATE'])\n",
    "pd_data['C'] = pd_data['FAILURE_DATE'] - pd_data['DATE']\n",
    "\n",
    "pd_data['TIME_TO_FAILURE'] = pd_data['C'] / np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up and sort the records by ID and DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data=pd_data.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_data=pd_data.sort_values(by=['ID', 'DATE'], ascending=[True, True])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new variable, FAILURE_TARGET.  It is equal to 1 if the record proceeds a failure by \"failure_window\" days or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_data['FAILURE_TARGET'] = np.where(((pd_data.TIME_TO_FAILURE < target_window) & ((pd_data.TIME_TO_FAILURE>=0))), 1, 0)\n",
    "\n",
    "pd_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_summed = pd_data.groupby(['FAILURE_TARGET'])['S5'].count()\n",
    "tips_summed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new field occurs about 4% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data['FAILURE_TARGET'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 11,740 target observations. This is better, but the data set is far from balanced. In the next section, we will use SMOTE to increase the number of failures synthetically. However, before we do that, let’s split our data into training, testing, and a validation sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create the Testing, Training and Validation Groupings <a id=\"groups\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are dealing with a panel data set (cross-sectional time-series), it is better not to take a random sample of all records. Doing so would put the records from one machine in all three sample data sets. To avoid this, we’ll randomly select IDs and place all of the records for each machine in either the training, testing, or validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get a Unique List of All IDs \n",
    "\n",
    "\n",
    "aa=pd_data\n",
    "\n",
    "pd_id=aa.drop_duplicates(subset='ID')\n",
    "pd_id=pd_id[['ID']]\n",
    "pd_id.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new variable with a random number between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_id['wookie'] = (np.random.randint(0, 10000, pd_id.shape[0]))/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "pd_id=pd_id[['ID', 'wookie']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give each record a 30% chance of being in the validation, a 35% chance of being in the testing and a 35% chance of being in the training data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_id['MODELING_GROUP'] = np.where(((pd_id.wookie <= 0.35)), 'TRAINING', np.where(((pd_id.wookie <= 0.65)), 'VALIDATION', 'TESTING'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many machines fall in each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_summed = pd_id.groupby(['MODELING_GROUP'])['wookie'].count()\n",
    "tips_summed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the Group of each id to each individual record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_data=pd_data.sort_values(by=['ID'], ascending=[True])\n",
    "pd_id=pd_id.sort_values(by=['ID'], ascending=[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd_data =pd_data.merge(pd_id, on=['ID'], how='inner')\n",
    "\n",
    "pd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many records are in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_summed = pd_data.groupby(['MODELING_GROUP'])['wookie'].count()\n",
    "tips_summed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how many failure targets are in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips_summed = pd_data.groupby(['MODELING_GROUP'])['FAILURE_TARGET'].sum()\n",
    "tips_summed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate data frame for the training data.  We will use this data set to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=pd_data[pd_data['MODELING_GROUP'] == 'TRAINING']\n",
    "df_training=df_training.drop(columns=['MODELING_GROUP','C','wookie','TIME_TO_FAILURE','flipper','START_DATE'])\n",
    "df_training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate data frame for the training and testing data sets.  We will use this to tweak our modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_test=pd_data[pd_data['MODELING_GROUP'] != 'VALIDATION']\n",
    "\n",
    "df_train_test=df_train_test.drop(columns=['wookie','TIME_TO_FAILURE','flipper','START_DATE'])\n",
    "df_train_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate data frame for all the data. We will use this to validate the model and comapre the accuracy of all groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total=pd_data.drop(columns=['C','wookie','TIME_TO_FAILURE','flipper','START_DATE'])\n",
    "df_total.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SMOTE the Training Data <a id=\"smote\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are only balancing the training data set. You may be asking why. Remember that our goal is to build a model the represents reality, right? When we SMOTE the data, we change the failure rate to 50%. This is no where near what we see in the actual machine data. Thus, goodness of fit statistics can be skewed. Because of this, it makes sense to build the model on the SMOTE data, but validate and test it on on the unaltered data. The unaltered data will be a better reflection of what to expect when you deploy the model to production.\n",
    "\n",
    "Define the Training features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_features=df_training[['REGION_CLUSTER','MAINTENANCE_VENDOR','MANUFACTURER','WELL_GROUP','AGE_OF_EQUIPMENT','S15','S17','S13','S5',\n",
    " 'S16','S19','S18','S8','S5_mean','S5_median','S5_max','S5_min','S13_mean','S13_median','S13_max','S13_min','S15_mean','S15_median',\n",
    " 'S15_max','S15_min','S16_mean','S16_median','S16_max','S16_min','S17_mean','S17_median','S17_max','S17_min','S18_mean','S18_median','S18_max','S18_min','S19_mean','S19_median','S19_max','S19_min',\n",
    " 'S5_chg','S13_chg','S15_chg','S16_chg','S17_chg','S18_chg','S19_chg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_target=df_training[['FAILURE_TARGET']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetically Balance the training data sets with a SMOTE algorithm. After we apply the SMOTE algorithm, we will have a balanced data set. 50% Failures and 50% Non-Failures. Note that this takes a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment these options if you want to expand the number of rows and columns that appear visually on the screen.\n",
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "#sm = SMOTE(random_state=12, ratio = 1.0)\n",
    "smx = SMOTENC(random_state=12,  categorical_features=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res, y_res = smx.fit_sample(training_features, training_target.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the SMOTE output back to complete data frames with independent and dependent variables.  Examine the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the Independent Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x=pd.DataFrame(x_res)\n",
    "\n",
    "df_x.columns = [\n",
    " 'REGION_CLUSTER','MAINTENANCE_VENDOR','MANUFACTURER','WELL_GROUP','AGE_OF_EQUIPMENT','S15','S17','S13','S5','S16','S19',\n",
    " 'S18','S8','S5_mean','S5_median','S5_max','S5_min','S13_mean','S13_median','S13_max','S13_min','S15_mean','S15_median','S15_max',\n",
    " 'S15_min','S16_mean','S16_median','S16_max','S16_min','S17_mean','S17_median','S17_max','S17_min','S18_mean','S18_median','S18_max','S18_min',\n",
    " 'S19_mean','S19_median','S19_max','S19_min','S5_chg','S13_chg','S15_chg','S16_chg','S17_chg','S18_chg','S19_chg']\n",
    "df_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y=pd.DataFrame(y_res)\n",
    "df_y.columns = ['FAILURE_TARGET']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the dependent variable is balanced.  It is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y.mean(axis = 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Merge the dependent and independent variables post SMOTE into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.concat([df_y, df_x], axis=1)\n",
    "df_balanced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 More data transformation and feature engineering <a id=\"more\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the categorical variables into binary dummy variables. We need to do this because the XGBT model (below) doesn't like categorical fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dv = pd.get_dummies(df_balanced['REGION_CLUSTER'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"A\": \"CLUSTER_A\",\"B\":\"CLUSTER_B\",\"C\":\"CLUSTER_C\",\"D\":\"CLUSTER_D\",\"E\":\"CLUSTER_E\",\"F\":\"CLUSTER_F\",\"G\":\"CLUSTER_G\",\"H\":\"CLUSTER_H\"})\n",
    "\n",
    "\n",
    "df_balanced= pd.concat([df_balanced, df_dv], axis=1)\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_balanced['MAINTENANCE_VENDOR'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"I\": \"MV_I\",\"J\":\"MV_J\",\"K\":\"MV_K\",\"L\":\"MV_L\",\"M\":\"MV_M\",\"N\":\"MV_N\",\"O\":\"MV_O\",\"P\":\"MV_P\"})\n",
    "\n",
    "\n",
    "df_balanced = pd.concat([df_balanced, df_dv], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_balanced['MANUFACTURER'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"Q\": \"MN_Q\",\"R\":\"MN_R\",\"S\":\"MN_S\",\"T\":\"MN_T\",\"U\":\"MN_U\",\"V\":\"MN_V\",\"W\":\"MN_W\",\"X\":\"MN_X\",\"Y\":\"MN_Y\",\"Z\":\"MN_Z\"})\n",
    "\n",
    "\n",
    "df_balanced = pd.concat([df_balanced, df_dv], axis=1)\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_balanced['WELL_GROUP'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={1: \"WG_1\",2:\"WG_2\",3:\"WG_3\",4:\"WG_4\",5:\"WG_5\",6:\"WG_6\",7:\"WG_7\",8:\"WG_8\"})\n",
    "\n",
    "\n",
    "df_balanced = pd.concat([df_balanced, df_dv], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dv = pd.get_dummies(df_train_test['REGION_CLUSTER'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"A\": \"CLUSTER_A\",\"B\":\"CLUSTER_B\",\"C\":\"CLUSTER_C\",\"D\":\"CLUSTER_D\",\"E\":\"CLUSTER_E\",\"F\":\"CLUSTER_F\",\"G\":\"CLUSTER_G\",\"H\":\"CLUSTER_H\"})\n",
    "\n",
    "\n",
    "df_train_test= pd.concat([df_train_test, df_dv], axis=1)\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_train_test['MAINTENANCE_VENDOR'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"I\": \"MV_I\",\"J\":\"MV_J\",\"K\":\"MV_K\",\"L\":\"MV_L\",\"M\":\"MV_M\",\"N\":\"MV_N\",\"O\":\"MV_O\",\"P\":\"MV_P\"})\n",
    "\n",
    "\n",
    "df_train_test = pd.concat([df_train_test, df_dv], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_train_test['MANUFACTURER'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"Q\": \"MN_Q\",\"R\":\"MN_R\",\"S\":\"MN_S\",\"T\":\"MN_T\",\"U\":\"MN_U\",\"V\":\"MN_V\",\"W\":\"MN_W\",\"X\":\"MN_X\",\"Y\":\"MN_Y\",\"Z\":\"MN_Z\"})\n",
    "\n",
    "\n",
    "df_train_test = pd.concat([df_train_test, df_dv], axis=1)\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_train_test['WELL_GROUP'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={1: \"WG_1\",2:\"WG_2\",3:\"WG_3\",4:\"WG_4\",5:\"WG_5\",6:\"WG_6\",7:\"WG_7\",8:\"WG_8\"})\n",
    "\n",
    "\n",
    "df_train_test = pd.concat([df_train_test, df_dv], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dv = pd.get_dummies(df_total['REGION_CLUSTER'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"A\": \"CLUSTER_A\",\"B\":\"CLUSTER_B\",\"C\":\"CLUSTER_C\",\"D\":\"CLUSTER_D\",\"E\":\"CLUSTER_E\",\"F\":\"CLUSTER_F\",\"G\":\"CLUSTER_G\",\"H\":\"CLUSTER_H\"})\n",
    "\n",
    "\n",
    "df_total= pd.concat([df_total, df_dv], axis=1)\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_total['MAINTENANCE_VENDOR'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"I\": \"MV_I\",\"J\":\"MV_J\",\"K\":\"MV_K\",\"L\":\"MV_L\",\"M\":\"MV_M\",\"N\":\"MV_N\",\"O\":\"MV_O\",\"P\":\"MV_P\"})\n",
    "\n",
    "\n",
    "df_total = pd.concat([df_total, df_dv], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_total['MANUFACTURER'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={\"Q\": \"MN_Q\",\"R\":\"MN_R\",\"S\":\"MN_S\",\"T\":\"MN_T\",\"U\":\"MN_U\",\"V\":\"MN_V\",\"W\":\"MN_W\",\"X\":\"MN_X\",\"Y\":\"MN_Y\",\"Z\":\"MN_Z\"})\n",
    "\n",
    "\n",
    "df_total = pd.concat([df_total, df_dv], axis=1)\n",
    "\n",
    "\n",
    "df_dv = pd.get_dummies(df_total['WELL_GROUP'])\n",
    "\n",
    "df_dv=df_dv.rename(columns={1: \"WG_1\",2:\"WG_2\",3:\"WG_3\",4:\"WG_4\",5:\"WG_5\",6:\"WG_6\",7:\"WG_7\",8:\"WG_8\"})\n",
    "\n",
    "\n",
    "df_total = pd.concat([df_total, df_dv], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.0 Build the model on the balanced training data set <a id=\"build\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the newly redundant categorical variables.  This are now represented by dummy variables.\n",
    "df_balanced=df_balanced.drop(columns=['REGION_CLUSTER','MAINTENANCE_VENDOR','MANUFACTURER','WELL_GROUP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the balanced data set, separate the dependent and independent variables to feed to the model development process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [x for x in df_balanced.columns if x not in ['FAILURE_TARGET','EQUIPMENT_FAILURE']]  \n",
    "dependent=pd.DataFrame(df_balanced['FAILURE_TARGET'])\n",
    "\n",
    "independent=df_balanced.drop(columns=['FAILURE_TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure everything is numeric for simplicity\n",
    "independent = independent.apply(pd.to_numeric) \n",
    "df_balanced = df_balanced.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def evaluate_model(alg, train, target, predictors,  early_stopping_rounds=1):\n",
    "    \n",
    "   \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train[predictors], target['FAILURE_TARGET'], eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(train[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(train[predictors])[:,1]\n",
    "    \n",
    "    feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False) \n",
    "    feat_imp.plot(kind='bar', title='Feature Importance', color='g') \n",
    "    plt.ylabel('Feature Importance Score')\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(target['FAILURE_TARGET'].values, dtrain_predictions))\n",
    "    print(\"AUC Score (Balanced): %f\" % metrics.roc_auc_score(target['FAILURE_TARGET'], dtrain_predprob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are initializing our model with default model parameters. Note that we could probably improve the results by tweaking the parameters, but we will save that exercise for another day. This step can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb0 = XGBClassifier(\n",
    " objective= 'binary:logistic')\n",
    "\n",
    "evaluate_model(xgb0, independent, dependent,features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a 50% cut-off on the balanced data, the accuracy is 84% and the AUC score is 91.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply the model to the unbalanced training and testing data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 Evaluate model on the unbalanced training and testing data set. <a id=\"score\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will assign a probability to fail for each record in the unbalanced training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test['P_FAIL']= xgb0.predict_proba(df_train_test[features])[:,1];\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a cut-off of .67 to categorize the p-value into a binary variable. If the probability of failing is greater than 67%, the record is a predicted failure. If the chance to fail is less than 67%, the record is a predicted non-failure. Note that determining the best cut-off is a bit art and a bit science. I will reserve a discussion on this for another day, but here are some techniques if you are interested.  \n",
    "https://medium.com/swlh/determining-a-cut-off-or-threshold-when-working-with-a-binary-dependent-target-variable-7c2342cf2a7c\n",
    "\n",
    "\n",
    "Also, split the training and testing records into two data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test['Y_FAIL'] = np.where(((df_train_test.P_FAIL <= .67)), 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing=df_train_test[df_train_test['MODELING_GROUP'] == 'TESTING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=df_train_test[df_train_test['MODELING_GROUP'] != 'TESTING']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a ROC Curve for Unbalanced Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold=metrics.roc_curve(df_testing['FAILURE_TARGET'], df_testing['P_FAIL'])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a ROC Curve for Unbalanced Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold=metrics.roc_curve(df_training['FAILURE_TARGET'], df_training['P_FAIL'])\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a cut-off of .67 the unblananced testing data set has an accuracy of 94% and AUC Score of 59%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model report:\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(df_testing['FAILURE_TARGET'].values, df_testing['Y_FAIL']))\n",
    "print(\"AUC Score (Test): %f\" % metrics.roc_auc_score(df_testing['FAILURE_TARGET'], df_testing['P_FAIL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a cut-off of .67 the unblananced training data set has an accuracy of 95% and AUC Score of 89%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model report:\n",
    "print(\"Accuracy : %.4g\" % metrics.accuracy_score(df_training['FAILURE_TARGET'].values, df_training['Y_FAIL']))\n",
    "print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(df_training['FAILURE_TARGET'], df_training['P_FAIL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the AUC curve, this model kind of stinks, huh? Let's take a look at the testing data with a confusion matrix.\n",
    "\n",
    "First, we will examine ('FAILURE_TARGET') the variable that is '1' if a date is within 28 days of a failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a confusion Matrix on the Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(df_testing.Y_FAIL, df_testing.FAILURE_TARGET, dropna=False))\n",
    "pd.crosstab(df_testing.Y_FAIL, df_testing.FAILURE_TARGET).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix doesn’t work, does it? Think about it. \n",
    "\n",
    "For example, let’s say that the machine fails on Sunday, and we have failure signals on Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, and Sunday. According to the matrix above, we would register six false positives and one true positive. In reality, we have one signal that occurred on seven consecutive days. These seven days should be counted as one true positive.\n",
    "\n",
    "Next, we will examine a confusion matrix with the original variable. A ‘1’ appears only on the days where a failure is registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.crosstab(df_testing.Y_FAIL, df_testing.EQUIPMENT_FAILURE, dropna=False))\n",
    "pd.crosstab(df_testing.Y_FAIL, df_testing.EQUIPMENT_FAILURE).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix doesn’t work either, does it?\n",
    "\n",
    "Think about it. A real positive only occurs if a failure happens on the same day as the signal. What if a signal happens on Monday, and it failed on Tuesday? I would count that as a true positive, but the confusion matrix above does not.\n",
    "\n",
    "So, we have some cleaning up to do. In the next section, we will build a more realistic confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.0 Business Rules and Heuristics <a id=\"bus\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is subjectivity when we create business rules and heuristics, just like when you fine-tune a model. For this reason, make sure you do your tweaking on the training data set and validate the results on the testing and validation data sets. This gives you the best chance of creating a result that will be reflective of an implementation.\n",
    "First, we will create a business rule that eliminates signals in “bunches.” For example, if there is a signal on Monday, Tuesday, and Wednesday, you likely don’t have three different problems. You more than probably have one problem identified on three consecutive days.\n",
    "\n",
    "To reduce the repetitive signals, we will create a business rule. This business rule will prohibit more than one signal every 90 days.\n",
    "\n",
    "Essentially we are saying that a failure signal means the equipment will fail in the next 90 days. Determining the window depends on the frequency of failure. If the equipment fails ten times a year, a 90-day window isn’t very useful. On the other hand, if the equipment fails once every five years, a three-month window is meaningful. Remember, the machines for this exercise are designed to last 4 to 6 years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_window=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data by id and date.\n",
    "xx=df_training\n",
    "xx=xx.sort_values(by=['ID','DATE'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our data is a panel, we must define the forecast window by machine ID. We want to prevent signals from one machine from \"bleeding\" into another machine. \n",
    "\n",
    "In other words, if a failure signal occurs on the last day a machine appears in the data, we don't want that signal to effect the subsequent machine.\n",
    "\n",
    "The following code ensures that each forecast window is specific to each machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a unique list of machines\n",
    "aa=xx\n",
    "\n",
    "pd_id=aa.drop_duplicates(subset='ID')\n",
    "pd_id=pd_id[['ID']]\n",
    "pd_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label each machine with a sequential number\n",
    "pd_id=pd_id.reset_index(drop=True)\n",
    "pd_id=pd_id.reset_index(drop=False)\n",
    "pd_id=pd_id.rename(columns={\"index\": \"SCOOBYDOO\"})\n",
    "pd_id['SCOOBYDOO']=pd_id['SCOOBYDOO']+1\n",
    "pd_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the max number of machines +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = pd_id[\"SCOOBYDOO\"]\n",
    "max_value = column.max()+1\n",
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append sequential number to main file\n",
    "xx=xx.sort_values(by=['ID'], ascending=[True])\n",
    "pd_id=pd_id.sort_values(by=['ID'], ascending=[True])\n",
    "xx =xx.merge(pd_id, on=['ID'], how='inner')\n",
    "xx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#sort data\n",
    "xx=xx.sort_values(by=['ID','DATE'], ascending=[True,True])\n",
    "\n",
    "#reset index\n",
    "xx=xx.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a null dataframe for the next step\n",
    "df_fred=xx\n",
    "df_fred['Y_FAIL_sumxx']=0\n",
    "df_fred=df_fred[df_fred['SCOOBYDOO'] == max_value+1]\n",
    "df_fred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step assigns a new failure indicator that incorporates the forecast window.  Note, this calulation occurs at a machine level.  This keeps a signal from one machine effecting another machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(max_value):\n",
    "        dffx=xx[xx['SCOOBYDOO'] ==x]\n",
    "        dff=dffx.copy()\n",
    "        dff['Y_FAIL_sumxx'] =(dff['Y_FAIL'].rolling(min_periods=1, window=(forecast_window)).sum())\n",
    "        df_fred= pd.concat([df_fred,dff])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=df_fred\n",
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx=dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a signal has occured in the last Z days, the signal is 0.\n",
    "xx['Y_FAILZ']=np.where((xx.Y_FAIL_sumxx>1), 0, xx.Y_FAIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's examine a confusion matrix with our new failure indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pd.crosstab(xx.Y_FAILZ, xx.EQUIPMENT_FAILURE, dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confusion matrix is more realistic, but we still have more work to do.\n",
    "\n",
    "A true positive occurs if a failure signal and an actual failure occur on the same day. For example, if a signal appears on Friday and the failure occurs on Saturday, this should be considered a true positive. The confusion matrix above currently counts this as a false positive.\n",
    "\n",
    "Now, we continue to apply business rules to make the results more realistic. First, we will create a unique id for each signal. This unique id will help us classify them as a true or false prediction of failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data by id and date.\n",
    "\n",
    "xx=xx.sort_values(by=['ID','DATE'], ascending=[True, True])\n",
    "\n",
    "#xx['bootie']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create signal id with the cumsum function.\n",
    "xx['SIGNAL_ID'] = xx['Y_FAILZ'].cumsum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will pull the records with a signal into a different data frame. \n",
    "\n",
    "Here we will create a new field that identifies the date of each signal (SIGNAL_DATE). \n",
    "\n",
    "Also, we will identify the ID Associated with each signal (ID_OF_SIGNAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_signals=xx[xx['Y_FAILZ'] == 1]\n",
    "df_signal_date=df_signals[['SIGNAL_ID','DATE','ID']]\n",
    "df_signal_date=df_signal_date.rename(index=str, columns={\"DATE\": \"SIGNAL_DATE\"})\n",
    "df_signal_date=df_signal_date.rename(index=str, columns={\"ID\": \"ID_OF_SIGNAL\"})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 302 signals.  Now each has a unique id.  Note that is still too many.  Below we will apply more heuristics to decrease the number of signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_signal_date.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the new fields to the working data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx =xx.merge(df_signal_date, on=['SIGNAL_ID'], how='outer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the fields we need to use going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx=xx[['DATE', 'ID', 'EQUIPMENT_FAILURE', 'FAILURE_TARGET','FAILURE_DATE',\n",
    "       'P_FAIL', 'Y_FAILZ','SIGNAL_ID',\n",
    "       'SIGNAL_DATE','ID_OF_SIGNAL']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a field called \"Warning\" that indicates the time from signal to failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "xx['C'] = xx['FAILURE_DATE'] - xx['SIGNAL_DATE']\n",
    "xx['WARNING'] = xx['C'] / np.timedelta64(1, 'D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace nan with 9999.  \n",
    "\n",
    "xx['WARNING'].fillna(9999, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  9.0 Define a True Positive, True Negative, False Positive and False Negative <a id=\"tp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a false positive, false negative, true positive, and true negative is one of the more essential steps in predictive maintenance models.  The problem's context drives these definitions, and there is no one size fits all approach. \n",
    "\n",
    "My definition makes sense here but is unique to this specific business problem.\n",
    "\n",
    "A true positive occurs if and only if the machine fails, and there was a signal within the last 90 days. Also, we have to ensure that the signal id belongs to the Well ID. Note that this prohibits a signal from another machine from being applied to the machine in question.\n",
    "\n",
    "A false negative occurs if and only if the machine fails, and it is not a true positive.\n",
    "\n",
    "A False Positive occurs if there is a failure signal, and a failure does not happen in the next 90 days. Also, if a signal occurs after the failure, this is a false positive. We also have to ensure that the signal ID belongs to the machine ID. Note that this prohibits a signal from another machine from being applied to the machine in question.\n",
    "\n",
    "If an observation is not a False Positive, a False Negative, or a True Positive, it is a True Negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a true positive\n",
    "xx['TRUE_POSITIVE'] = np.where(((xx.EQUIPMENT_FAILURE == 1) & (xx.WARNING<=forecast_window) &(xx.WARNING>=0) & (xx.ID_OF_SIGNAL==xx.ID)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a false negative\n",
    "xx['FALSE_NEGATIVE'] = np.where((xx.TRUE_POSITIVE==0) & (xx.EQUIPMENT_FAILURE==1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define a false positive\n",
    "xx['BAD_S']=np.where((xx.WARNING<0) | (xx.WARNING>=forecast_window), 1, 0)\n",
    "\n",
    "xx['FALSE_POSITIVE'] = np.where(((xx.Y_FAILZ == 1) & (xx.BAD_S==1) & (xx.ID_OF_SIGNAL==xx.ID)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx['bootie']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx['MODELING_GROUP']='TRAINING'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Create the final Cross-Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xx['CATEGORY']=np.where((xx.FALSE_POSITIVE==1),'FALSE_POSITIVE',\n",
    "                                      (np.where((xx.FALSE_NEGATIVE==1),'FALSE_NEGATIVE',\n",
    "                                                (np.where((xx.TRUE_POSITIVE==1),'TRUE_POSITIVE','TRUE_NEGATIVE')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table = pd.pivot_table(xx, values=['bootie'], index=['MODELING_GROUP'],columns=['CATEGORY'], aggfunc=np.sum)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previously described definitions of a false positive, false negative, true positive and true negative in the training data set there are:\n",
    "    \n",
    "    41 False Negatives\n",
    "    107 False Positives\n",
    "    106361 True Negatives and\n",
    "    105 True Postives.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.0 Apply Model and Heuristics the Testing and Validation Data Sets. <a id=\"apply\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the probability of failure for all records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['P_FAIL']= xgb0.predict_proba(df_total[features])[:,1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a predicted failure indicator based on a cut-off of .67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total['Y_FAIL'] = np.where(((df_total.P_FAIL <= .67)), 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=df_total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that failure indicator occurs only once every 120 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=yy\n",
    "\n",
    "pd_id=aa.drop_duplicates(subset='ID')\n",
    "pd_id=pd_id[['ID']]\n",
    "pd_id.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_id=pd_id.reset_index(drop=True)\n",
    "pd_id=pd_id.reset_index(drop=False)\n",
    "pd_id=pd_id.rename(columns={\"index\": \"SCOOBYDOO\"})\n",
    "pd_id['SCOOBYDOO']=pd_id['SCOOBYDOO']+1\n",
    "pd_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column = pd_id[\"SCOOBYDOO\"]\n",
    "max_value = column.max()+1\n",
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yy=yy.sort_values(by=['ID'], ascending=[True])\n",
    "pd_id=pd_id.sort_values(by=['ID'], ascending=[True])\n",
    "yy =yy.merge(pd_id, on=['ID'], how='inner')\n",
    "yy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=yy.sort_values(by=['ID','DATE'], ascending=[True,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=yy.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fred=yy\n",
    "df_fred['Y_FAIL_sumxx']=0\n",
    "df_fred=df_fred[df_fred['SCOOBYDOO'] == max_value+1]\n",
    "df_fred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(max_value):\n",
    "        dffx=yy[yy['SCOOBYDOO'] ==x]\n",
    "        dff=dffx.copy()\n",
    "        dff['Y_FAIL_sumxx'] =(dff['Y_FAIL'].rolling(min_periods=1, window=(forecast_window)).sum())\n",
    "        df_fred= pd.concat([df_fred,dff])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=df_fred\n",
    "dff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy['Y_FAILZ']=np.where((yy.Y_FAIL_sumxx>1), 0, yy.Y_FAIL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the WARNING Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=yy.sort_values(by=['ID','DATE'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a signal id\n",
    "yy['SIGNAL_ID'] = yy['Y_FAILZ'].cumsum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the signal date and ID_OF_SIGNAL\n",
    "\n",
    "yy_signals=yy[yy['Y_FAILZ'] == 1]\n",
    "yy_signal_date=yy_signals[['SIGNAL_ID','DATE','ID']]\n",
    "yy_signal_date=yy_signal_date.rename(index=str, columns={\"DATE\": \"SIGNAL_DATE\"})\n",
    "yy_signal_date=yy_signal_date.rename(index=str, columns={\"ID\": \"ID_OF_SIGNAL\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the two data frames back into one.\n",
    "\n",
    "yy =yy.merge(yy_signal_date, on=['SIGNAL_ID'], how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep on the fields we need\n",
    "yy=yy[['DATE', 'ID', 'EQUIPMENT_FAILURE', 'FAILURE_TARGET','FAILURE_DATE','MODELING_GROUP',\n",
    "       'P_FAIL', 'Y_FAILZ','SIGNAL_ID',\n",
    "       'SIGNAL_DATE','ID_OF_SIGNAL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the warning time between each failure date and signal date.\n",
    "yy['C'] = yy['FAILURE_DATE'] - yy['SIGNAL_DATE']\n",
    "yy['WARNING'] = yy['C'] / np.timedelta64(1, 'D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define True Positives, True Negatives, False Positives and False Negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy['WARNING'].fillna(9999, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a true positive\n",
    "yy['TRUE_POSITIVE'] = np.where(((yy.EQUIPMENT_FAILURE == 1) & (yy.WARNING<=forecast_window) &(yy.WARNING>=0) & (yy.ID_OF_SIGNAL==yy.ID)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a false negative\n",
    "yy['FALSE_NEGATIVE'] = np.where((yy.TRUE_POSITIVE==0) & (yy.EQUIPMENT_FAILURE==1), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a false positive\n",
    "yy['BAD_S']=np.where((yy.WARNING<0) | (yy.WARNING>=forecast_window), 1, 0)\n",
    "\n",
    "yy['FALSE_POSITIVE'] = np.where(((yy.Y_FAILZ == 1) & (yy.BAD_S==1) & (yy.ID_OF_SIGNAL==yy.ID)), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy['bootie']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy['CATEGORY']=np.where((yy.FALSE_POSITIVE==1),'FALSE_POSITIVE',\n",
    "                                      (np.where((yy.FALSE_NEGATIVE==1),'FALSE_NEGATIVE',\n",
    "                                                (np.where((yy.TRUE_POSITIVE==1),'TRUE_POSITIVE','TRUE_NEGATIVE')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define metrics for the Testing, Training and Validation Data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.pivot_table(yy, values=['bootie'], index=['MODELING_GROUP'],columns=['CATEGORY'], aggfunc=np.sum)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when I introduced the use case I presented this chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/4800/1*5Ar2n3ZHMwVeOhgXI_dm0w.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A false positive is “Unnecessary Maintenance.” \n",
    "A true positive is a “Timely and Appropriate Maintenance.” \n",
    "A false negative is “Machine Runs to Failure.” \n",
    "\n",
    "This means that a false positive costs $1,500.   \n",
    "\n",
    "A false negative costs $30,000.   \n",
    "\n",
    "A true positive costs $7,500. \n",
    "\n",
    "A true negative has no cost because no action is taken.\n",
    "\n",
    "Now we can calculate the total cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy['TOTAL_COST']=yy.FALSE_NEGATIVE*30000+yy.FALSE_POSITIVE*1500+yy.TRUE_POSITIVE*7500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the costs by modeling group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table = pd.pivot_table(yy, values=['TOTAL_COST'],index=['MODELING_GROUP'], aggfunc=np.sum)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the number of machines per modelling group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells=yy[['ID','MODELING_GROUP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells=wells.drop_duplicates(subset='ID')\n",
    "\n",
    "wells.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = wells.groupby(['MODELING_GROUP'])['ID'].count()\n",
    "wells=pd.DataFrame(wells)\n",
    "wells=wells.rename(columns={\"ID\": \"WELLS\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the total costs and total machines into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = yy.groupby(['MODELING_GROUP'])['TOTAL_COST'].sum()\n",
    "tc=pd.DataFrame(tc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average cost per machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac =tc.merge(wells, on=['MODELING_GROUP'], how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac['AVERAGE_COST']=ac.TOTAL_COST/ac.WELLS\n",
    "ac['LIFT']=28000-ac.AVERAGE_COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  11.0 Conclusions <a id=\"conc\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It currently costs the firm about 28,000 dollars per machine in the current data set. By deploying a PM solution, we can lower those costs to between 22,000 and 23,000 dollars. This equates to a savings of about 4,500 per machine. For all 419 machines, this is a total savings of approximately 1,885,000 dollars.\n",
    "\n",
    "Not too shabby.\n",
    "\n",
    "One final note. There are many judgments I made that work for this example but may not work for you. Unfortunately, there is no “one size fits all” solution for any data science problem.\n",
    "\n",
    "Nonetheless, this exercise should give you a useful reference as you approach these types of problems in the future.\n",
    "\n",
    "As far as the next steps, I would encourage you to see if you can improve the solution by optimizing the model. Maybe incorporate some hyper-parameter optimization or even try a different model. Let me know how it turns out!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Shad Griffin**, is a Data Scientist at the IBM Global Solution Center in Dallas, Texas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright &copy; IBM Corp. 2020. This notebook and its source code are released under the terms of the MIT License.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
